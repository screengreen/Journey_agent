"""Self-RAG –≥—Ä–∞—Ñ –Ω–∞ LangGraph. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç InputData."""

from __future__ import annotations

import json
from typing import List, Literal, Optional, Tuple, TypedDict, Union

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from src.vdb.config import OPENAI_API_KEY, OPENAI_MODEL, MAX_ITERATIONS
from src.models.event import Event
from src.vdb.rag.memory import check_memory
from src.vdb.rag.prompts import (
    QUERY_REFORMULATION_PROMPT,
    RELEVANCE_EVALUATION_PROMPT,
)
from src.vdb.rag.retriever import EventRetriever
from src.planner_agent.models import InputData, Constraints


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è extraction constraints ---
CONSTRAINTS_EXTRACTION_PROMPT = """
–ò–∑–≤–ª–µ–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç (–±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π, –±–µ–∑ markdown), —Å—Ç—Ä–æ–≥–æ —Å –∫–ª—é—á–∞–º–∏:
- start_time: "HH:MM" –∏–ª–∏ null
- end_time: "HH:MM" –∏–ª–∏ null
- max_total_time_minutes: integer –∏–ª–∏ null
- preferred_transport: string –∏–ª–∏ null
- budget: number –∏–ª–∏ null
- other_constraints: array[string]

–¢–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}
""".strip()


class SelfRAGState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞ Self-RAG."""

    user_query: str
    owner: Optional[str]

    # retrieval loop
    retrieved_events: List[Event]
    reformulated_queries: List[str]
    iteration_count: int
    current_query: str

    # flags
    memory_found: bool
    is_relevant: bool

    # output
    constraints: Optional[Constraints]
    response: Optional[InputData]

    # logs
    logs: List[str]


# ---------------- Nodes ----------------

def check_memory_node(state: SelfRAGState) -> SelfRAGState:
    """–£–∑–µ–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏ (–Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç –≥—Ä–∞—Ñ, —Ç–æ–ª—å–∫–æ –ª–æ–≥–∏—Ä—É–µ—Ç)."""
    has_memory = check_memory(state["user_query"], state.get("owner"))
    logs = state.get("logs", [])
    logs.append(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏: {'–Ω–∞–π–¥–µ–Ω–æ' if has_memory else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}")

    return {
        **state,
        "memory_found": has_memory,
        # is_relevant —Ç—É—Ç –ù–ï —Ç—Ä–æ–≥–∞–µ–º ‚Äî —ç—Ç–æ –ø—Ä–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å retrieved_events
        "logs": logs,
    }


def retrieve_events_node(state: SelfRAGState, retriever: EventRetriever) -> SelfRAGState:
    """–£–∑–µ–ª –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π."""
    query = state.get("current_query") or state["user_query"]
    owner = state.get("owner")

    events = retriever.retrieve(query, owner=owner)

    logs = state.get("logs", [])
    logs.append(f"üîé –ü–æ–∏—Å–∫ —Å–æ–±—ã—Ç–∏–π: –∑–∞–ø—Ä–æ—Å='{query}', –≤–ª–∞–¥–µ–ª–µ—Ü='{owner}', –Ω–∞–π–¥–µ–Ω–æ={len(events)}")

    return {
        **state,
        "retrieved_events": events,
        "logs": logs,
    }


def evaluate_relevance_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    if retriever is None:
        retriever = EventRetriever()

    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = RELEVANCE_EVALUATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    relevance_text = response.content.strip().upper()
    is_relevant = relevance_text.startswith("YES")

    logs = state.get("logs", [])
    logs.append(f"üìä –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {relevance_text} ({'—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ' if is_relevant else '–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ'})")
    logs.append(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(state['retrieved_events'])}")

    return {
        **state,
        "is_relevant": is_relevant,
        "logs": logs,
    }


def reformulate_queries_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤."""
    if retriever is None:
        retriever = EventRetriever()

    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = QUERY_REFORMULATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    reformulated_text = response.content.strip()

    new_queries = [q.strip() for q in reformulated_text.split("\n") if q.strip()]
    current_query = new_queries[0] if new_queries else state["user_query"]

    logs = state.get("logs", [])
    iteration = state.get("iteration_count", 0) + 1
    logs.append(f"üîÑ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration}):")
    for i, q in enumerate(new_queries[:3], 1):
        logs.append(f"   {i}. {q}")

    return {
        **state,
        "reformulated_queries": state.get("reformulated_queries", []) + new_queries,
        "current_query": current_query,
        "iteration_count": iteration,
        "logs": logs,
    }


def extract_constraints_node(state: SelfRAGState, llm: BaseChatModel) -> SelfRAGState:
    """–î–æ—Å—Ç–∞—ë–º Constraints –∏–∑ user_query —á–µ—Ä–µ–∑ LLM (JSON), —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º fallback."""
    logs = state.get("logs", [])

    prompt = CONSTRAINTS_EXTRACTION_PROMPT.format(user_query=state["user_query"])
    raw = ""
    constraints = None

    try:
        resp = llm.invoke(prompt)
        raw = (resp.content or "").strip()

        # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å—ë-—Ç–∞–∫–∏ –∑–∞–≤–µ—Ä–Ω—É–ª–∞ –≤ ```...```
        raw = raw.removeprefix("```json").removeprefix("```").removesuffix("```").strip()

        data = json.loads(raw)

        # pydantic v1/v2 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        if hasattr(Constraints, "model_validate"):
            constraints = Constraints.model_validate(data)
        else:
            constraints = Constraints.parse_obj(data)

        logs.append("üß© Constraints –∏–∑–≤–ª–µ—á–µ–Ω—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ LLM")
    except Exception:
        constraints = Constraints()
        logs.append("üß© –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å Constraints —á–µ—Ä–µ–∑ LLM ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é –ø—É—Å—Ç—ã–µ Constraints()")
        if raw:
            logs.append(f"   (—Å—ã—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–æ–¥–µ–ª–∏: {raw[:200]}...)")

    return {
        **state,
        "constraints": constraints,
        "logs": logs,
    }


def build_input_data_node(state: SelfRAGState) -> SelfRAGState:
    """–°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π InputData."""
    logs = state.get("logs", [])
    constraints = state.get("constraints") or Constraints()

    input_data = InputData(
        events=state.get("retrieved_events", []),
        user_prompt=state["user_query"],
        constraints=constraints,
    )

    logs.append("‚úÖ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω InputData")

    return {
        **state,
        "response": input_data,
        "logs": logs,
    }


# ---------------- Conditions ----------------

def should_reformulate_or_finish(state: SelfRAGState) -> Literal["reformulate", "finish"]:
    """–ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º, –∏–Ω–∞—á–µ —Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º."""
    iteration_count = state.get("iteration_count", 0)
    is_relevant = state.get("is_relevant", False)

    if is_relevant or iteration_count >= MAX_ITERATIONS:
        return "finish"
    return "reformulate"


# ---------------- Graph factory ----------------

def create_self_rag_graph(
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
) -> Tuple[StateGraph, Optional[EventRetriever]]:
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ Self-RAG."""
    if llm is None:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        llm = ChatOpenAI(model=OPENAI_MODEL, api_key=OPENAI_API_KEY, temperature=0)

    created_retriever = None
    if retriever is None:
        retriever = EventRetriever()
        created_retriever = retriever

    workflow = StateGraph(SelfRAGState)

    workflow.add_node("check_memory", check_memory_node)
    workflow.add_node("retrieve_events", lambda state: retrieve_events_node(state, retriever))
    workflow.add_node("evaluate_relevance", lambda state: evaluate_relevance_node(state, llm, retriever))
    workflow.add_node("reformulate_queries", lambda state: reformulate_queries_node(state, llm, retriever))
    workflow.add_node("extract_constraints", lambda state: extract_constraints_node(state, llm))
    workflow.add_node("build_input_data", build_input_data_node)

    # Flow:
    # check_memory -> retrieve -> evaluate -> (reformulate loop) -> extract_constraints -> build_input_data -> END
    workflow.set_entry_point("check_memory")
    workflow.add_edge("check_memory", "retrieve_events")
    workflow.add_edge("retrieve_events", "evaluate_relevance")

    workflow.add_conditional_edges(
        "evaluate_relevance",
        should_reformulate_or_finish,
        {
            "reformulate": "reformulate_queries",
            "finish": "extract_constraints",
        },
    )

    workflow.add_edge("reformulate_queries", "retrieve_events")
    workflow.add_edge("extract_constraints", "build_input_data")
    workflow.add_edge("build_input_data", END)

    return workflow.compile(), created_retriever


# ---------------- Runner ----------------

def run_self_rag(
    user_query: str,
    owner: Optional[str] = None,
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
    return_logs: bool = False,
) -> Union[InputData, Tuple[InputData, List[str]]]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç Self-RAG –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç InputData (–∏–ª–∏ InputData + –ª–æ–≥–∏)."""
    graph, created_retriever = create_self_rag_graph(llm=llm, retriever=retriever)

    initial_state: SelfRAGState = {
        "user_query": user_query,
        "owner": owner,

        "retrieved_events": [],
        "reformulated_queries": [],
        "iteration_count": 0,
        "current_query": user_query,

        "memory_found": False,
        "is_relevant": False,

        "constraints": None,
        "response": None,

        "logs": [],
    }

    try:
        result = graph.invoke(initial_state)

        input_data: Optional[InputData] = result.get("response")
        logs: List[str] = result.get("logs", [])

        if input_data is None:
            # –Ω–∞ –≤—Å—è–∫–∏–π –ø–æ–∂–∞—Ä–Ω—ã–π, —á—Ç–æ–±—ã —Ç–∏–ø –≤—Å–µ–≥–¥–∞ –±—ã–ª InputData
            input_data = InputData(
                events=result.get("retrieved_events", []),
                user_prompt=user_query,
                constraints=Constraints(),
            )
            logs.append("‚ö†Ô∏è response –±—ã–ª None ‚Üí —Å–æ–±—Ä–∞–ª InputData fallback")

        if return_logs:
            return input_data, logs
        return input_data

    finally:
        if created_retriever is not None:
            created_retriever.close()
        if retriever is not None:
            retriever.close()
