"""Self-RAG –≥—Ä–∞—Ñ –Ω–∞ LangGraph. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç InputData."""

from __future__ import annotations

import json
from typing import List, Literal, Optional, Tuple, TypedDict, Union

from langchain_core.language_models.chat_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from src.vdb.config import OPENAI_API_KEY, OPENAI_MODEL, MAX_ITERATIONS
from src.models.event import Event
from src.vdb.rag.memory import check_memory
from src.vdb.rag.prompts import (
    CITY_EXTRACTION_PROMPT,
    QUERY_REFORMULATION_PROMPT,
    RELEVANCE_EVALUATION_PROMPT,
)
from src.vdb.rag.retriever import EventRetriever
from src.planner_agent.models import InputData, Constraints


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è extraction constraints ---
CONSTRAINTS_EXTRACTION_PROMPT = """
–ò–∑–≤–ª–µ–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π JSON –æ–±—ä–µ–∫—Ç (–±–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π, –±–µ–∑ markdown), —Å—Ç—Ä–æ–≥–æ —Å –∫–ª—é—á–∞–º–∏:
- start_time: "HH:MM" –∏–ª–∏ null (–Ω–∞–ø—Ä–∏–º–µ—Ä: "10:00", "14:30")
- end_time: "HH:MM" –∏–ª–∏ null (–Ω–∞–ø—Ä–∏–º–µ—Ä: "18:00", "20:00")
- max_total_time_minutes: integer –∏–ª–∏ null (–æ–±—â–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –ø–ª–∞–Ω–∞ –≤ –º–∏–Ω—É—Ç–∞—Ö)
- preferred_transport: string –∏–ª–∏ null (–Ω–∞–ø—Ä–∏–º–µ—Ä: "walking", "bus", "car")
- budget: number –∏–ª–∏ null (–±—é–¥–∂–µ—Ç –≤ —Ä—É–±–ª—è—Ö)
- max_events: integer –∏–ª–∏ null (—Å–∫–æ–ª—å–∫–æ —Å–æ–±—ã—Ç–∏–π/–º–µ—Å—Ç –º–∞–∫—Å–∏–º—É–º –≤–∫–ª—é—á–∏—Ç—å –≤ –ø–ª–∞–Ω)
- other_constraints: array[string] (–¥—Ä—É–≥–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è)

–í–ê–ñ–ù–û –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ max_events:
- –ï—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —è–≤–Ω–æ —É–∫–∞–∑–∞–ª –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Å—Ç ("2 –º–µ—Å—Ç–∞", "3 —Å–æ–±—ã—Ç–∏—è", "–ø–∞—Ä—É –º–µ—Å—Ç", "–Ω–µ–º–Ω–æ–≥–æ –º–µ—Å—Ç") - –∏–∑–≤–ª–µ–∫–∏ —ç—Ç–æ —á–∏—Å–ª–æ
- "–ø–∞—Ä—É" = 2, "–Ω–µ—Å–∫–æ–ª—å–∫–æ" = 3, "–Ω–µ–º–Ω–æ–≥–æ" = 3-4
- –ï—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ù–ï —É–∫–∞–∑–∞–Ω–æ - –≤–µ—Ä–Ω–∏ null
- –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ "–º–Ω–æ–≥–æ" –∏–ª–∏ "–º–∞–∫—Å–∏–º—É–º" - –≤–µ—Ä–Ω–∏ null (–Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º)

–í–ê–ñ–ù–û –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –≤—Ä–µ–º–µ–Ω–∏:
- –ò–∑–≤–ª–µ–∫–∞–π –¢–û–õ–¨–ö–û —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
- –§–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏ –°–¢–†–û–ì–û "HH:MM" (–Ω–∞–ø—Ä–∏–º–µ—Ä: "09:00", "14:30", "18:00")
- –ï—Å–ª–∏ –≤—Ä–µ–º—è –Ω–µ —É–∫–∞–∑–∞–Ω–æ - –≤–µ—Ä–Ω–∏ null
- "—É—Ç—Ä–æ–º" = null (–Ω–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–µ –≤—Ä–µ–º—è), "—Å 10 —É—Ç—Ä–∞" = "10:00"

–¢–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}
""".strip()


class SelfRAGState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞ Self-RAG."""

    user_query: str
    owner: Optional[str]

    # extracted city for filtering public events
    city: Optional[str]

    # retrieval loop
    retrieved_events: List[Event]
    reformulated_queries: List[str]
    iteration_count: int
    current_query: str

    # flags
    memory_found: bool
    is_relevant: bool

    # output
    constraints: Optional[Constraints]
    response: Optional[InputData]

    # logs
    logs: List[str]


# ---------------- Nodes ----------------

def check_memory_node(state: SelfRAGState) -> SelfRAGState:
    """–£–∑–µ–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏ (–Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–µ—Ç –≥—Ä–∞—Ñ, —Ç–æ–ª—å–∫–æ –ª–æ–≥–∏—Ä—É–µ—Ç)."""
    has_memory = check_memory(state["user_query"], state.get("owner"))
    logs = state.get("logs", [])
    logs.append(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏: {'–Ω–∞–π–¥–µ–Ω–æ' if has_memory else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}")

    return {
        **state,
        "memory_found": has_memory,
        # is_relevant —Ç—É—Ç –ù–ï —Ç—Ä–æ–≥–∞–µ–º ‚Äî —ç—Ç–æ –ø—Ä–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å retrieved_events
        "logs": logs,
    }


def extract_city_node(state: SelfRAGState, llm: BaseChatModel) -> SelfRAGState:
    """–£–∑–µ–ª –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    logs = state.get("logs", [])

    prompt = CITY_EXTRACTION_PROMPT.format_messages(user_query=state["user_query"])

    try:
        response = llm.invoke(prompt)
        city_text = (response.content or "").strip()

        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ null –∏–ª–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É, –≥–æ—Ä–æ–¥ –Ω–µ –Ω–∞–π–¥–µ–Ω
        if city_text.lower() in ("null", "", "none", "–Ω–µ —É–∫–∞–∑–∞–Ω"):
            city = None
            logs.append("üèôÔ∏è –ì–æ—Ä–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ")
        else:
            city = city_text
            logs.append(f"üèôÔ∏è –ò–∑–≤–ª–µ—á—ë–Ω –≥–æ—Ä–æ–¥: {city}")
    except Exception as e:
        city = None
        logs.append(f"üèôÔ∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –≥–æ—Ä–æ–¥–∞: {e}")

    return {
        **state,
        "city": city,
        "logs": logs,
    }


def retrieve_events_node(state: SelfRAGState, retriever: EventRetriever) -> SelfRAGState:
    """–£–∑–µ–ª –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π."""
    query = state.get("current_query") or state["user_query"]
    owner = state.get("owner")
    city = state.get("city")

    events = retriever.retrieve(query, owner=owner, city=city)

    logs = state.get("logs", [])
    city_info = f", –≥–æ—Ä–æ–¥='{city}'" if city else ""
    logs.append(f"üîé –ü–æ–∏—Å–∫ —Å–æ–±—ã—Ç–∏–π: –∑–∞–ø—Ä–æ—Å='{query}', –≤–ª–∞–¥–µ–ª–µ—Ü='{owner}'{city_info}, –Ω–∞–π–¥–µ–Ω–æ={len(events)}")

    return {
        **state,
        "retrieved_events": events,
        "logs": logs,
    }


def evaluate_relevance_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    if retriever is None:
        retriever = EventRetriever()

    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = RELEVANCE_EVALUATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    relevance_text = response.content.strip().upper()
    is_relevant = relevance_text.startswith("YES")

    logs = state.get("logs", [])
    logs.append(f"üìä –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {relevance_text} ({'—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ' if is_relevant else '–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ'})")
    logs.append(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(state['retrieved_events'])}")

    return {
        **state,
        "is_relevant": is_relevant,
        "logs": logs,
    }


def reformulate_queries_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤."""
    if retriever is None:
        retriever = EventRetriever()

    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = QUERY_REFORMULATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    reformulated_text = response.content.strip()

    new_queries = [q.strip() for q in reformulated_text.split("\n") if q.strip()]
    current_query = new_queries[0] if new_queries else state["user_query"]

    logs = state.get("logs", [])
    iteration = state.get("iteration_count", 0) + 1
    logs.append(f"üîÑ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration}):")
    for i, q in enumerate(new_queries[:3], 1):
        logs.append(f"   {i}. {q}")

    return {
        **state,
        "reformulated_queries": state.get("reformulated_queries", []) + new_queries,
        "current_query": current_query,
        "iteration_count": iteration,
        "logs": logs,
    }


def extract_constraints_node(state: SelfRAGState, llm: BaseChatModel) -> SelfRAGState:
    """–î–æ—Å—Ç–∞—ë–º Constraints –∏–∑ user_query —á–µ—Ä–µ–∑ LLM (JSON), —Å –±–µ–∑–æ–ø–∞—Å–Ω—ã–º fallback."""
    from datetime import time as time_type
    
    logs = state.get("logs", [])

    prompt = CONSTRAINTS_EXTRACTION_PROMPT.format(user_query=state["user_query"])
    raw = ""
    constraints = None

    try:
        resp = llm.invoke(prompt)
        raw = (resp.content or "").strip()

        # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å—ë-—Ç–∞–∫–∏ –∑–∞–≤–µ—Ä–Ω—É–ª–∞ –≤ ```...```
        raw = raw.removeprefix("```json").removeprefix("```").removesuffix("```").strip()

        data = json.loads(raw)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–æ–∫–æ–≤–æ–µ –≤—Ä–µ–º—è –≤ –æ–±—ä–µ–∫—Ç—ã time
        if data.get("start_time") and isinstance(data["start_time"], str):
            try:
                hours, minutes = data["start_time"].split(":")
                data["start_time"] = time_type(int(hours), int(minutes))
            except (ValueError, AttributeError):
                logs.append(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å start_time: {data['start_time']}")
                data["start_time"] = None
        
        if data.get("end_time") and isinstance(data["end_time"], str):
            try:
                hours, minutes = data["end_time"].split(":")
                data["end_time"] = time_type(int(hours), int(minutes))
            except (ValueError, AttributeError):
                logs.append(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å end_time: {data['end_time']}")
                data["end_time"] = None

        # pydantic v1/v2 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
        if hasattr(Constraints, "model_validate"):
            constraints = Constraints.model_validate(data)
        else:
            constraints = Constraints.parse_obj(data)

        # –õ–æ–≥–∏—Ä—É–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ constraints
        constraint_details = []
        if constraints.start_time:
            constraint_details.append(f"start_time={constraints.start_time}")
        if constraints.end_time:
            constraint_details.append(f"end_time={constraints.end_time}")
        if constraints.max_events:
            constraint_details.append(f"max_events={constraints.max_events}")
        if constraints.max_total_time_minutes:
            constraint_details.append(f"max_total_time={constraints.max_total_time_minutes}–º–∏–Ω")
        
        if constraint_details:
            logs.append(f"üß© Constraints –∏–∑–≤–ª–µ—á–µ–Ω—ã: {', '.join(constraint_details)}")
        else:
            logs.append("üß© Constraints –∏–∑–≤–ª–µ—á–µ–Ω—ã (–ø—É—Å—Ç—ã–µ)")
            
    except Exception as e:
        constraints = Constraints()
        logs.append(f"üß© –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å Constraints —á–µ—Ä–µ–∑ LLM ‚Üí –∏—Å–ø–æ–ª—å–∑—É—é –ø—É—Å—Ç—ã–µ Constraints() (–æ—à–∏–±–∫–∞: {str(e)[:100]})")
        if raw:
            logs.append(f"   (—Å—ã—Ä–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–æ–¥–µ–ª–∏: {raw[:200]}...)")

    return {
        **state,
        "constraints": constraints,
        "logs": logs,
    }


def build_input_data_node(state: SelfRAGState) -> SelfRAGState:
    """–°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π InputData."""
    logs = state.get("logs", [])
    constraints = state.get("constraints") or Constraints()

    input_data = InputData(
        events=state.get("retrieved_events", []),
        user_prompt=state["user_query"],
        constraints=constraints,
    )

    logs.append("‚úÖ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω InputData")

    return {
        **state,
        "response": input_data,
        "logs": logs,
    }


# ---------------- Conditions ----------------

def should_reformulate_or_finish(state: SelfRAGState) -> Literal["reformulate", "finish"]:
    """–ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º, –∏–Ω–∞—á–µ —Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–µ–º."""
    iteration_count = state.get("iteration_count", 0)
    is_relevant = state.get("is_relevant", False)

    if is_relevant or iteration_count >= MAX_ITERATIONS:
        return "finish"
    return "reformulate"


# ---------------- Graph factory ----------------

def create_self_rag_graph(
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
) -> Tuple[StateGraph, Optional[EventRetriever]]:
    """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ Self-RAG."""
    if llm is None:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        llm = ChatOpenAI(model=OPENAI_MODEL, api_key=OPENAI_API_KEY, temperature=0)

    created_retriever = None
    if retriever is None:
        retriever = EventRetriever()
        created_retriever = retriever

    workflow = StateGraph(SelfRAGState)

    workflow.add_node("check_memory", check_memory_node)
    workflow.add_node("extract_city", lambda state: extract_city_node(state, llm))
    workflow.add_node("retrieve_events", lambda state: retrieve_events_node(state, retriever))
    workflow.add_node("evaluate_relevance", lambda state: evaluate_relevance_node(state, llm, retriever))
    workflow.add_node("reformulate_queries", lambda state: reformulate_queries_node(state, llm, retriever))
    workflow.add_node("extract_constraints", lambda state: extract_constraints_node(state, llm))
    workflow.add_node("build_input_data", build_input_data_node)

    # Flow:
    # check_memory -> extract_city -> retrieve -> evaluate -> (reformulate loop) -> extract_constraints -> build_input_data -> END
    workflow.set_entry_point("check_memory")
    workflow.add_edge("check_memory", "extract_city")
    workflow.add_edge("extract_city", "retrieve_events")
    workflow.add_edge("retrieve_events", "evaluate_relevance")

    workflow.add_conditional_edges(
        "evaluate_relevance",
        should_reformulate_or_finish,
        {
            "reformulate": "reformulate_queries",
            "finish": "extract_constraints",
        },
    )

    workflow.add_edge("reformulate_queries", "retrieve_events")
    workflow.add_edge("extract_constraints", "build_input_data")
    workflow.add_edge("build_input_data", END)

    return workflow.compile(), created_retriever


# ---------------- Runner ----------------

def run_self_rag(
    user_query: str,
    owner: Optional[str] = None,
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
    return_logs: bool = False,
) -> Union[InputData, Tuple[InputData, List[str]]]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç Self-RAG –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç InputData (–∏–ª–∏ InputData + –ª–æ–≥–∏)."""
    graph, created_retriever = create_self_rag_graph(llm=llm, retriever=retriever)

    initial_state: SelfRAGState = {
        "user_query": user_query,
        "owner": owner,
        "city": None,

        "retrieved_events": [],
        "reformulated_queries": [],
        "iteration_count": 0,
        "current_query": user_query,

        "memory_found": False,
        "is_relevant": False,

        "constraints": None,
        "response": None,

        "logs": [],
    }

    try:
        result = graph.invoke(initial_state)

        input_data: Optional[InputData] = result.get("response")
        logs: List[str] = result.get("logs", [])

        if input_data is None:
            # –Ω–∞ –≤—Å—è–∫–∏–π –ø–æ–∂–∞—Ä–Ω—ã–π, —á—Ç–æ–±—ã —Ç–∏–ø –≤—Å–µ–≥–¥–∞ –±—ã–ª InputData
            input_data = InputData(
                events=result.get("retrieved_events", []),
                user_prompt=user_query,
                constraints=Constraints(),
            )
            logs.append("‚ö†Ô∏è response –±—ã–ª None ‚Üí —Å–æ–±—Ä–∞–ª InputData fallback")

        if return_logs:
            return input_data, logs
        return input_data

    finally:
        if created_retriever is not None:
            created_retriever.close()
        if retriever is not None:
            retriever.close()
