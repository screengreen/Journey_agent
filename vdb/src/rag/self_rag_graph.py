"""Self-RAG –≥—Ä–∞—Ñ –Ω–∞ LangGraph."""

from typing import List, Literal, Optional, Tuple, TypedDict, Union

from langchain_core.language_models import BaseChatModel
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph

from src.config import OPENAI_API_KEY, OPENAI_MODEL, MAX_ITERATIONS
from src.models.event import Event
from src.rag.memory import check_memory
from src.rag.prompts import (
    QUERY_REFORMULATION_PROMPT,
    RELEVANCE_EVALUATION_PROMPT,
    RESPONSE_GENERATION_PROMPT,
)
from src.rag.retriever import EventRetriever


class SelfRAGState(TypedDict):
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –≥—Ä–∞—Ñ–∞ Self-RAG."""

    user_query: str
    user_tag: Optional[str]
    retrieved_events: List[Event]
    reformulated_queries: List[str]
    is_relevant: bool
    response: Optional[str]
    iteration_count: int
    current_query: str  # –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–º)
    logs: List[str]  # –õ–æ–≥–∏ —Ä–µ—à–µ–Ω–∏–π –≥—Ä–∞—Ñ–∞


def check_memory_node(state: SelfRAGState) -> SelfRAGState:
    """–£–∑–µ–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏."""
    has_memory = check_memory(state["user_query"], state.get("user_tag"))
    logs = state.get("logs", [])
    logs.append(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏: {'–Ω–∞–π–¥–µ–Ω–æ' if has_memory else '–Ω–µ –Ω–∞–π–¥–µ–Ω–æ'}")
    return {
        **state,
        "is_relevant": has_memory,
        "logs": logs,
    }


def retrieve_events_node(state: SelfRAGState, retriever: EventRetriever) -> SelfRAGState:
    """–£–∑–µ–ª –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π."""
    query = state.get("current_query", state["user_query"])
    user_tag = state.get("user_tag")

    events = retriever.retrieve(query, user_tag=user_tag)
    
    logs = state.get("logs", [])
    logs.append(f"üîé –ü–æ–∏—Å–∫ —Å–æ–±—ã—Ç–∏–π: –∑–∞–ø—Ä–æ—Å='{query}', —Ç–µ–≥='{user_tag}', –Ω–∞–π–¥–µ–Ω–æ={len(events)}")

    return {
        **state,
        "retrieved_events": events,
        "logs": logs,
    }


def evaluate_relevance_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –æ—Ü–µ–Ω–∫–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏–∑–≤–ª–µ—á–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏."""
    if retriever is None:
        retriever = EventRetriever()
    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = RELEVANCE_EVALUATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    relevance_text = response.content.strip().upper()

    is_relevant = relevance_text.startswith("YES")
    
    logs = state.get("logs", [])
    logs.append(f"üìä –û—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏: {relevance_text} ({'—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ' if is_relevant else '–Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ'})")
    logs.append(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(state['retrieved_events'])}")

    return {
        **state,
        "is_relevant": is_relevant,
        "logs": logs,
    }


def reformulate_queries_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤."""
    if retriever is None:
        retriever = EventRetriever()
    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = QUERY_REFORMULATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    reformulated_text = response.content.strip()

    # –ü–∞—Ä—Å–∏–º –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–∫–∞–∂–¥—ã–π –Ω–∞ –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–µ)
    new_queries = [
        q.strip() for q in reformulated_text.split("\n") if q.strip()
    ]

    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
    current_query = new_queries[0] if new_queries else state["user_query"]
    
    logs = state.get("logs", [])
    iteration = state.get("iteration_count", 0) + 1
    logs.append(f"üîÑ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ (–∏—Ç–µ—Ä–∞—Ü–∏—è {iteration}):")
    for i, q in enumerate(new_queries[:3], 1):
        logs.append(f"   {i}. {q}")

    return {
        **state,
        "reformulated_queries": state.get("reformulated_queries", []) + new_queries,
        "current_query": current_query,
        "iteration_count": iteration,
        "logs": logs,
    }


def generate_response_node(
    state: SelfRAGState, llm: BaseChatModel, retriever: Optional[EventRetriever] = None
) -> SelfRAGState:
    """–£–∑–µ–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
    if retriever is None:
        retriever = EventRetriever()
    events_context = retriever.format_events_for_context(state["retrieved_events"])

    prompt = RESPONSE_GENERATION_PROMPT.format_messages(
        user_query=state["user_query"],
        retrieved_events=events_context,
    )

    response = llm.invoke(prompt)
    
    logs = state.get("logs", [])
    logs.append(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

    return {
        **state,
        "response": response.content,
        "logs": logs,
    }


def should_retrieve(state: SelfRAGState) -> Literal["retrieve", "generate"]:
    """–£—Å–ª–æ–≤–∏–µ: –Ω—É–∂–Ω–æ –ª–∏ –∏—Å–∫–∞—Ç—å —Å–æ–±—ã—Ç–∏—è –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."""
    if state.get("is_relevant", False):
        return "generate"
    return "retrieve"


def should_reformulate(state: SelfRAGState) -> Literal["reformulate", "generate"]:
    """–£—Å–ª–æ–≤–∏–µ: –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –∑–∞–ø—Ä–æ—Å –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."""
    iteration_count = state.get("iteration_count", 0)
    is_relevant = state.get("is_relevant", False)

    # –ï—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ –∏–ª–∏ –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∏—Ç–µ—Ä–∞—Ü–∏–π - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
    if is_relevant or iteration_count >= MAX_ITERATIONS:
        return "generate"
    return "reformulate"


def create_self_rag_graph(
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
) -> Tuple[StateGraph, Optional[EventRetriever]]:
    """
    –°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ Self-RAG.

    Args:
        llm: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤–∞—è)
        retriever: Retriever –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π (–µ—Å–ª–∏ None, —Å–æ–∑–¥–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π)

    Returns:
        –ö–æ—Ä—Ç–µ–∂ (–≥—Ä–∞—Ñ LangGraph, retriever –¥–ª—è –∑–∞–∫—Ä—ã—Ç–∏—è)
    """
    if llm is None:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        llm = ChatOpenAI(model=OPENAI_MODEL, api_key=OPENAI_API_KEY, temperature=0)

    created_retriever = None
    if retriever is None:
        retriever = EventRetriever()
        created_retriever = retriever

    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ
    workflow = StateGraph(SelfRAGState)

    # –î–æ–±–∞–≤–ª—è–µ–º —É–∑–ª—ã
    workflow.add_node("check_memory", check_memory_node)
    workflow.add_node(
        "retrieve_events",
        lambda state: retrieve_events_node(state, retriever),
    )
    workflow.add_node(
        "evaluate_relevance",
        lambda state: evaluate_relevance_node(state, llm, retriever),
    )
    workflow.add_node(
        "reformulate_queries",
        lambda state: reformulate_queries_node(state, llm, retriever),
    )
    workflow.add_node(
        "generate_response",
        lambda state: generate_response_node(state, llm, retriever),
    )

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–µ—Ä–µ—Ö–æ–¥—ã
    workflow.set_entry_point("check_memory")

    # –ü–æ—Å–ª–µ check_memory: –µ—Å–ª–∏ –ø–∞–º—è—Ç—å –Ω–∞–π–¥–µ–Ω–∞ -> generate_response, –∏–Ω–∞—á–µ -> retrieve_events
    workflow.add_conditional_edges(
        "check_memory",
        should_retrieve,
        {
            "retrieve": "retrieve_events",
            "generate": "generate_response",
        },
    )

    # –ü–æ—Å–ª–µ retrieve_events -> evaluate_relevance
    workflow.add_edge("retrieve_events", "evaluate_relevance")

    # –ü–æ—Å–ª–µ evaluate_relevance: –µ—Å–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ -> generate_response, –∏–Ω–∞—á–µ -> reformulate_queries
    workflow.add_conditional_edges(
        "evaluate_relevance",
        should_reformulate,
        {
            "reformulate": "reformulate_queries",
            "generate": "generate_response",
        },
    )

    # –ü–æ—Å–ª–µ reformulate_queries -> retrieve_events (—Å –Ω–æ–≤—ã–º –∑–∞–ø—Ä–æ—Å–æ–º)
    workflow.add_edge("reformulate_queries", "retrieve_events")

    # generate_response -> END
    workflow.add_edge("generate_response", END)

    return workflow.compile(), created_retriever


def run_self_rag(
    user_query: str,
    user_tag: Optional[str] = None,
    llm: Optional[BaseChatModel] = None,
    retriever: Optional[EventRetriever] = None,
    return_logs: bool = False,
) -> Union[str, Tuple[str, List[str]]]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç Self-RAG —Å–∏—Å—Ç–µ–º—É.

    Args:
        user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_tag: –¢–µ–≥ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–æ–±—ã—Ç–∏–π
        llm: –Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        retriever: Retriever (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        return_logs: –ï—Å–ª–∏ True, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ—Ä—Ç–µ–∂ (–æ—Ç–≤–µ—Ç, –ª–æ–≥–∏)

    Returns:
        –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å–∏—Å—Ç–µ–º—ã –∏–ª–∏ –∫–æ—Ä—Ç–µ–∂ (–æ—Ç–≤–µ—Ç, –ª–æ–≥–∏) –µ—Å–ª–∏ return_logs=True
    """
    graph, created_retriever = create_self_rag_graph(llm=llm, retriever=retriever)

    initial_state: SelfRAGState = {
        "user_query": user_query,
        "user_tag": user_tag,
        "retrieved_events": [],
        "reformulated_queries": [],
        "is_relevant": False,
        "response": None,
        "iteration_count": 0,
        "current_query": user_query,
        "logs": [],
    }

    try:
        result = graph.invoke(initial_state)
        
        response = result.get("response", "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç.")
        logs = result.get("logs", [])
        
        if return_logs:
            return response, logs
        return response
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate, –µ—Å–ª–∏ retriever –±—ã–ª —Å–æ–∑–¥–∞–Ω –≤–Ω—É—Ç—Ä–∏
        if created_retriever is not None:
            created_retriever.close()
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate
        if retriever is not None:
            retriever.close()
        elif hasattr(graph, '_retriever'):
            # –ï—Å–ª–∏ retriever –±—ã–ª —Å–æ–∑–¥–∞–Ω –≤–Ω—É—Ç—Ä–∏ –≥—Ä–∞—Ñ–∞, –Ω—É–∂–Ω–æ –µ–≥–æ –∑–∞–∫—Ä—ã—Ç—å
            pass

